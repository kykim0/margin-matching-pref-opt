# Model arguments
model_name_or_path: kykim0/llama3-8b-ultrachat-sft-itt
model_revision: main
torch_dtype: bfloat16
use_flash_attention_2: true
trust_remote_code: true

# Data training arguments
dataset_mixer:
  allenai/ultrafeedback_binarized_cleaned: 1.0
dataset_splits:
- train_prefs
- test_prefs
preprocessing_num_workers: 12

# Reward modeling config (4 GPUs assumed)
bf16: true
do_eval: true
eval_steps: 100
evaluation_strategy: steps
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
optim: adamw_torch
lr_scheduler_type: linear
learning_rate: 3.0e-06
weight_decay: 0.0
log_level: info
logging_steps: 10
logging_strategy: steps
max_length: 1024
max_steps: -1
num_train_epochs: 2
run_name: capo-rm
output_dir: save/l38b-itt-uf-capo-rm
bt_beta: null
resume_from_checkpoint: false
overwrite_output_dir: false
per_device_train_batch_size: 1
per_device_eval_batch_size: 8
push_to_hub: false
hub_model_id: null
hub_strategy: every_save
remove_unused_columns: false
report_to:
- wandb
hub_token: null
save_strategy: epoch
save_total_limit: null
save_only_model: true
seed: 42
